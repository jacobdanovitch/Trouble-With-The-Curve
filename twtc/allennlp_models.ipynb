{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deprecated cache directory found (/home/jacobgdt/.allennlp/datasets).  Please remove this directory from your system to free up space.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacobgdt/anaconda3/envs/cling/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import *\n",
    "from model import build_model\n",
    "\n",
    "from constants import USE_GPU, DATA_ROOT, ENCODERS, config_defaults, FEATURE_COLS\n",
    "from config import Config\n",
    "from test import calculate_metrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5824, 74)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>report</th>\n",
       "      <th>Arm</th>\n",
       "      <th>Changeup</th>\n",
       "      <th>Control</th>\n",
       "      <th>Curveball</th>\n",
       "      <th>Cutter</th>\n",
       "      <th>Fastball</th>\n",
       "      <th>Field</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_RF</th>\n",
       "      <th>pos_RHP</th>\n",
       "      <th>pos_SS</th>\n",
       "      <th>pos_A</th>\n",
       "      <th>pos_A+</th>\n",
       "      <th>pos_A-</th>\n",
       "      <th>pos_AA</th>\n",
       "      <th>pos_AAA</th>\n",
       "      <th>pos_R</th>\n",
       "      <th>pos_UNK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**Luke Heimlich</td>\n",
       "      <td>22.3</td>\n",
       "      <td>PERSON is a Level NUMBER sex offender and woul...</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A.J. Alexy</td>\n",
       "      <td>18.7</td>\n",
       "      <td>PERSON made headlines for all the wrong reason...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A.J. Cole</td>\n",
       "      <td>22.0</td>\n",
       "      <td>The ORGANIZATION have acquired PERSON twice, f...</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A.J. Cole</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Signed for an above-slot $NUMBER million as a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A.J. Cole</td>\n",
       "      <td>21.0</td>\n",
       "      <td>It often takes time for those high-ceilinged, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              name   age                                             report  \\\n",
       "0  **Luke Heimlich  22.3  PERSON is a Level NUMBER sex offender and woul...   \n",
       "1       A.J. Alexy  18.7  PERSON made headlines for all the wrong reason...   \n",
       "2        A.J. Cole  22.0  The ORGANIZATION have acquired PERSON twice, f...   \n",
       "3        A.J. Cole  24.0  Signed for an above-slot $NUMBER million as a ...   \n",
       "4        A.J. Cole  21.0  It often takes time for those high-ceilinged, ...   \n",
       "\n",
       "   Arm  Changeup  Control  Curveball  Cutter  Fastball  Field   ...    pos_RF  \\\n",
       "0    0        55       55         55       0        55      0   ...         0   \n",
       "1    0        50       50         55       0        55      0   ...         0   \n",
       "2    0        55       55          0       0        70      0   ...         0   \n",
       "3    0        55       55         45       0        55      0   ...         0   \n",
       "4    0        50       50          0       0        70      0   ...         0   \n",
       "\n",
       "   pos_RHP  pos_SS  pos_A  pos_A+  pos_A- pos_AA pos_AAA pos_R pos_UNK  \n",
       "0        0       0      0       0       0      0       1     0       0  \n",
       "1        1       0      0       0       0      0       0     1       0  \n",
       "2        1       0      0       0       0      0       1     0       0  \n",
       "3        1       0      0       0       0      0       1     0       0  \n",
       "4        1       0      0       0       0      1       0     0       0  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../notebooks/preprocessed.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4672\n",
       "1    1152\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacobgdt/anaconda3/envs/cling/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5500, 1966)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('train.csv').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models.biattentive_classification_network import BiattentiveClassificationNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using csv reader.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacobgdt/anaconda3/envs/cling/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2961: DtypeWarning: Columns (17,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "5497it [00:00, 3748957.58it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5497/5497 [00:00<00:00, 11986.90it/s]\n"
     ]
    }
   ],
   "source": [
    "#f_cols = pd.read_csv('train.csv').drop(columns=['name', 'report']).columns.tolist()\n",
    "f_cols = FEATURE_COLS+[str(i) for i in range(1892)]\n",
    "\n",
    "reader = MLBDataReader(feature_cols=f_cols, standardize=True)\n",
    "\n",
    "ds = reader.read('train.csv')\n",
    "vocab = Vocabulary.from_instances(ds, max_vocab_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BiattentiveClassificationNetwork in module allennlp.models.biattentive_classification_network:\n",
      "\n",
      "class BiattentiveClassificationNetwork(allennlp.models.model.Model)\n",
      " |  This class implements the Biattentive Classification Network model described\n",
      " |  in section 5 of `Learned in Translation: Contextualized Word Vectors (NIPS 2017)\n",
      " |  <https://arxiv.org/abs/1708.00107>`_ for text classification. We assume we're\n",
      " |  given a piece of text, and we predict some output label.\n",
      " |  \n",
      " |  At a high level, the model starts by embedding the tokens and running them through\n",
      " |  a feed-forward neural net (``pre_encode_feedforward``). Then, we encode these\n",
      " |  representations with a ``Seq2SeqEncoder`` (``encoder``). We run biattention\n",
      " |  on the encoder output representations (self-attention in this case, since\n",
      " |  the two representations that typically go into biattention are identical) and\n",
      " |  get out an attentive vector representation of the text. We combine this text\n",
      " |  representation with the encoder outputs computed earlier, and then run this through\n",
      " |  yet another ``Seq2SeqEncoder`` (the ``integrator``). Lastly, we take the output of the\n",
      " |  integrator and max, min, mean, and self-attention pool to create a final representation,\n",
      " |  which is passed through a maxout network or some feed-forward layers\n",
      " |  to output a classification (``output_layer``).\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  vocab : ``Vocabulary``, required\n",
      " |      A Vocabulary, required in order to compute sizes for input/output projections.\n",
      " |  text_field_embedder : ``TextFieldEmbedder``, required\n",
      " |      Used to embed the ``tokens`` ``TextField`` we get as input to the model.\n",
      " |  embedding_dropout : ``float``\n",
      " |      The amount of dropout to apply on the embeddings.\n",
      " |  pre_encode_feedforward : ``FeedForward``\n",
      " |      A feedforward network that is run on the embedded tokens before they\n",
      " |      are passed to the encoder.\n",
      " |  encoder : ``Seq2SeqEncoder``\n",
      " |      The encoder to use on the tokens.\n",
      " |  integrator : ``Seq2SeqEncoder``\n",
      " |      The encoder to use when integrating the attentive text encoding\n",
      " |      with the token encodings.\n",
      " |  integrator_dropout : ``float``\n",
      " |      The amount of dropout to apply on integrator output.\n",
      " |  output_layer : ``Union[Maxout, FeedForward]``\n",
      " |      The maxout or feed forward network that takes the final representations and produces\n",
      " |      a classification prediction.\n",
      " |  elmo : ``Elmo``, optional (default=``None``)\n",
      " |      If provided, will be used to concatenate pretrained ELMo representations to\n",
      " |      either the integrator output (``use_integrator_output_elmo``) or the\n",
      " |      input (``use_input_elmo``).\n",
      " |  use_input_elmo : ``bool`` (default=``False``)\n",
      " |      If true, concatenate pretrained ELMo representations to the input vectors.\n",
      " |  use_integrator_output_elmo : ``bool`` (default=``False``)\n",
      " |      If true, concatenate pretrained ELMo representations to the integrator output.\n",
      " |  initializer : ``InitializerApplicator``, optional (default=``InitializerApplicator()``)\n",
      " |      Used to initialize the model parameters.\n",
      " |  regularizer : ``RegularizerApplicator``, optional (default=``None``)\n",
      " |      If provided, will be used to calculate the regularization penalty during training.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BiattentiveClassificationNetwork\n",
      " |      allennlp.models.model.Model\n",
      " |      torch.nn.modules.module.Module\n",
      " |      allennlp.common.registrable.Registrable\n",
      " |      allennlp.common.from_params.FromParams\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab:allennlp.data.vocabulary.Vocabulary, text_field_embedder:allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder, embedding_dropout:float, pre_encode_feedforward:allennlp.modules.feedforward.FeedForward, encoder:allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, integrator:allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder, integrator_dropout:float, output_layer:Union[allennlp.modules.feedforward.FeedForward, allennlp.modules.maxout.Maxout], elmo:allennlp.modules.elmo.Elmo, use_input_elmo:bool=False, use_integrator_output_elmo:bool=False, initializer:allennlp.nn.initializers.InitializerApplicator=<allennlp.nn.initializers.InitializerApplicator object at 0x7faf11597630>, regularizer:Union[allennlp.nn.regularizers.regularizer_applicator.RegularizerApplicator, NoneType]=None) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decode(self, output_dict:Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]\n",
      " |      Does a simple argmax over the class probabilities, converts indices to string labels, and\n",
      " |      adds a ``\"label\"`` key to the dictionary with the result.\n",
      " |  \n",
      " |  forward(self, tokens:Dict[str, torch.LongTensor], label:torch.LongTensor=None) -> Dict[str, torch.Tensor]\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tokens : Dict[str, torch.LongTensor], required\n",
      " |          The output of ``TextField.as_array()``.\n",
      " |      label : torch.LongTensor, optional (default = None)\n",
      " |          A variable representing the label for each instance in the batch.\n",
      " |      Returns\n",
      " |      -------\n",
      " |      An output dictionary consisting of:\n",
      " |      class_probabilities : torch.FloatTensor\n",
      " |          A tensor of shape ``(batch_size, num_classes)`` representing a\n",
      " |          distribution over the label classes for each instance.\n",
      " |      loss : torch.FloatTensor, optional\n",
      " |          A scalar loss to be optimised.\n",
      " |  \n",
      " |  get_metrics(self, reset:bool=False) -> Dict[str, float]\n",
      " |      Returns a dictionary of metrics. This method will be called by\n",
      " |      :class:`allennlp.training.Trainer` in order to compute and use model metrics for early\n",
      " |      stopping and model serialization.  We return an empty dictionary here rather than raising\n",
      " |      as it is not required to implement metrics for a new model.  A boolean `reset` parameter is\n",
      " |      passed, as frequently a metric accumulator will have some state which should be reset\n",
      " |      between epochs. This is also compatible with :class:`~allennlp.training.Metric`s. Metrics\n",
      " |      should be populated during the call to ``forward``, with the\n",
      " |      :class:`~allennlp.training.Metric` handling the accumulation of the metric until this\n",
      " |      method is called.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_params(vocab:allennlp.data.vocabulary.Vocabulary, params:allennlp.common.params.Params) -> 'BiattentiveClassificationNetwork' from builtins.type\n",
      " |      This is the automatic implementation of `from_params`. Any class that subclasses `FromParams`\n",
      " |      (or `Registrable`, which itself subclasses `FromParams`) gets this implementation for free.\n",
      " |      If you want your class to be instantiated from params in the \"obvious\" way -- pop off parameters\n",
      " |      and hand them to your constructor with the same names -- this provides that functionality.\n",
      " |      \n",
      " |      If you need more complex logic in your from `from_params` method, you'll have to implement\n",
      " |      your own method that overrides this one.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from allennlp.models.model.Model:\n",
      " |  \n",
      " |  extend_embedder_vocab(self, embedding_sources_mapping:Dict[str, str]=None) -> None\n",
      " |      Iterates through all embedding modules in the model and assures it can embed\n",
      " |      with the extended vocab. This is required in fine-tuning or transfer learning\n",
      " |      scenarios where model was trained with original vocabulary but during\n",
      " |      fine-tuning/transfer-learning, it will have it work with extended vocabulary\n",
      " |      (original + new-data vocabulary).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      embedding_sources_mapping : Dict[str, str], (optional, default=None)\n",
      " |          Mapping from model_path to pretrained-file path of the embedding\n",
      " |          modules. If pretrained-file used at time of embedding initialization\n",
      " |          isn't available now, user should pass this mapping. Model path is\n",
      " |          path traversing the model attributes upto this embedding module.\n",
      " |          Eg. \"_text_field_embedder.token_embedder_tokens\".\n",
      " |  \n",
      " |  forward_on_instance(self, instance:allennlp.data.instance.Instance) -> Dict[str, numpy.ndarray]\n",
      " |      Takes an :class:`~allennlp.data.instance.Instance`, which typically has raw text in it,\n",
      " |      converts that text into arrays using this model's :class:`Vocabulary`, passes those arrays\n",
      " |      through :func:`self.forward()` and :func:`self.decode()` (which by default does nothing)\n",
      " |      and returns the result.  Before returning the result, we convert any\n",
      " |      ``torch.Tensors`` into numpy arrays and remove the batch dimension.\n",
      " |  \n",
      " |  forward_on_instances(self, instances:List[allennlp.data.instance.Instance]) -> List[Dict[str, numpy.ndarray]]\n",
      " |      Takes a list of  :class:`~allennlp.data.instance.Instance`s, converts that text into\n",
      " |      arrays using this model's :class:`Vocabulary`, passes those arrays through\n",
      " |      :func:`self.forward()` and :func:`self.decode()` (which by default does nothing)\n",
      " |      and returns the result.  Before returning the result, we convert any\n",
      " |      ``torch.Tensors`` into numpy arrays and separate the\n",
      " |      batched output into a list of individual dicts per instance. Note that typically\n",
      " |      this will be faster on a GPU (and conditionally, on a CPU) than repeated calls to\n",
      " |      :func:`forward_on_instance`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      instances : List[Instance], required\n",
      " |          The instances to run the model on.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A list of the models output for each instance.\n",
      " |  \n",
      " |  get_parameters_for_histogram_tensorboard_logging(self) -> List[str]\n",
      " |      Returns the name of model parameters used for logging histograms to tensorboard.\n",
      " |  \n",
      " |  get_regularization_penalty(self) -> Union[float, torch.Tensor]\n",
      " |      Computes the regularization penalty for the model.\n",
      " |      Returns 0 if the model was not configured to use regularization.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from allennlp.models.model.Model:\n",
      " |  \n",
      " |  load(config:allennlp.common.params.Params, serialization_dir:str, weights_file:str=None, cuda_device:int=-1) -> 'Model' from builtins.type\n",
      " |      Instantiates an already-trained model, based on the experiment\n",
      " |      configuration and some optional overrides.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      config: Params\n",
      " |          The configuration that was used to train the model. It should definitely\n",
      " |          have a `model` section, and should probably have a `trainer` section\n",
      " |          as well.\n",
      " |      serialization_dir: str = None\n",
      " |          The directory containing the serialized weights, parameters, and vocabulary\n",
      " |          of the model.\n",
      " |      weights_file: str = None\n",
      " |          By default we load the weights from `best.th` in the serialization\n",
      " |          directory, but you can override that value here.\n",
      " |      cuda_device: int = -1\n",
      " |          By default we load the model on the CPU, but if you want to load it\n",
      " |          for GPU usage you can specify the id of your GPU here\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model: Model\n",
      " |          The model specified in the configuration, loaded with the serialized\n",
      " |          vocabulary and the trained weights.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from allennlp.models.model.Model:\n",
      " |  \n",
      " |  __annotations__ = {'_warn_for_unseparable_batches': typing.Set[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> def init_weights(m):\n",
      " |                  print(m)\n",
      " |                  if type(m) == nn.Linear:\n",
      " |                      m.weight.data.fill_(1.0)\n",
      " |                      print(m.weight)\n",
      " |      \n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should reimplement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point desired :attr:`dtype` s. In addition, this method will\n",
      " |      only cast the floating point parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point type of\n",
      " |              the floating point parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from allennlp.common.registrable.Registrable:\n",
      " |  \n",
      " |  by_name(name:str) -> Type[~T] from builtins.type\n",
      " |  \n",
      " |  list_available() -> List[str] from builtins.type\n",
      " |      List default first if it exists\n",
      " |  \n",
      " |  register(name:str) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from allennlp.common.registrable.Registrable:\n",
      " |  \n",
      " |  default_implementation = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(BiattentiveClassificationNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 6 required positional arguments: 'pre_encode_feedforward', 'encoder', 'integrator', 'integrator_dropout', 'output_layer', and 'elmo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-38f5c28c16f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtext_field_embedder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0membedding_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 6 required positional arguments: 'pre_encode_feedforward', 'encoder', 'integrator', 'integrator_dropout', 'output_layer', and 'elmo'"
     ]
    }
   ],
   "source": [
    "bcn = BiattentiveClassificationNetwork(\n",
    "    vocab=vocab,\n",
    "    text_field_embedder=model.word_embeddings,\n",
    "    embedding_dropout=0,\n",
    "    pre_encode_feedforward=None,\n",
    "    encoder=None,\n",
    "    integrator=None,\n",
    "    integrator_dropout=0,\n",
    "    output_layer=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicTextFieldEmbedder(\n",
       "  (token_embedder_tokens): Embedding()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CnnEncoder as encoder.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.4767 ||: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:53<00:00, 12.91it/s]\n",
      "loss: 0.3646 ||: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:52<00:00, 13.19it/s]\n",
      "loss: 0.2941 ||: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:52<00:00, 13.18it/s]\n",
      "loss: 0.2208 ||: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:51<00:00, 13.26it/s]\n",
      "loss: 0.1591 ||: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:51<00:00, 13.27it/s]\n",
      "loss: 0.1042 ||: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:51<00:00, 13.27it/s]\n",
      "loss: 0.0675 ||: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:52<00:00, 13.22it/s]\n",
      "loss: 0.0403 ||: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:51<00:00, 13.27it/s]\n",
      "loss: 0.0239 ||: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:52<00:00, 13.18it/s]\n",
      "loss: 0.0135 ||: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 688/688 [00:51<00:00, 13.27it/s]\n"
     ]
    }
   ],
   "source": [
    "config = Config(**config_defaults, track=False, indexer='base', model='base', encoder='cnn', features='union')\n",
    "config.epochs=3\n",
    "config.batch_size=4\n",
    "config.learning_rate=0.01\n",
    "\n",
    "\n",
    "model, trainer, vocab = build_model(config, reader, ds, vocab)\n",
    "metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacobgdt/anaconda3/envs/cling/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2961: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using csv reader.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "583it [00:00, 2409142.10it/s]\n"
     ]
    }
   ],
   "source": [
    "test_ds = reader.read('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83cc523aa7864fd286e19edc6a70872a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution\n",
      "---\n",
      "0    526\n",
      "1     57\n",
      "dtype: int64\n",
      "---\n",
      "Balanced accuracy: 0.6449\n",
      "Binary F-1: 0.4419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      minors       0.85      0.96      0.90       468\n",
      "         mlb       0.67      0.33      0.44       115\n",
      "\n",
      "    accuracy                           0.84       583\n",
      "   macro avg       0.76      0.64      0.67       583\n",
      "weighted avg       0.82      0.84      0.81       583\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, acc, f1 = calculate_metrics(model, test_ds, vocab, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Arm</th>\n",
       "      <th>Changeup</th>\n",
       "      <th>Control</th>\n",
       "      <th>Curveball</th>\n",
       "      <th>Cutter</th>\n",
       "      <th>Fastball</th>\n",
       "      <th>Field</th>\n",
       "      <th>Hit</th>\n",
       "      <th>Power</th>\n",
       "      <th>...</th>\n",
       "      <th>pos_RF</th>\n",
       "      <th>pos_RHP</th>\n",
       "      <th>pos_SS</th>\n",
       "      <th>pos_A</th>\n",
       "      <th>pos_A+</th>\n",
       "      <th>pos_A-</th>\n",
       "      <th>pos_AA</th>\n",
       "      <th>pos_AAA</th>\n",
       "      <th>pos_R</th>\n",
       "      <th>pos_UNK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.3</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.7</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  Arm  Changeup  Control  Curveball  Cutter  Fastball  Field  Hit  \\\n",
       "0  22.3    0        55       55         55       0        55      0    0   \n",
       "1  18.7    0        50       50         55       0        55      0    0   \n",
       "2  22.0    0        55       55          0       0        70      0    0   \n",
       "3  24.0    0        55       55         45       0        55      0    0   \n",
       "4  21.0    0        50       50          0       0        70      0    0   \n",
       "\n",
       "   Power   ...     pos_RF  pos_RHP  pos_SS  pos_A  pos_A+  pos_A-  pos_AA  \\\n",
       "0      0   ...          0        0       0      0       0       0       0   \n",
       "1      0   ...          0        1       0      0       0       0       0   \n",
       "2      0   ...          0        1       0      0       0       0       0   \n",
       "3      0   ...          0        1       0      0       0       0       0   \n",
       "4      0   ...          0        1       0      0       0       0       1   \n",
       "\n",
       "   pos_AAA  pos_R  pos_UNK  \n",
       "0        1      0        0  \n",
       "1        0      1        0  \n",
       "2        1      0        0  \n",
       "3        1      0        0  \n",
       "4        0      0        0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[FEATURE_COLS].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
